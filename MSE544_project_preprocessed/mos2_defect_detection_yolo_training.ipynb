{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be7fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r yolov5/requirements.txt --user \n",
    "#%pip install scikit-learn scikit-image azureml-core --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35125bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import labeledImage, normalize_coordinates, convert_to_yolo_format\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, shutil, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63b195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the root folder that contains images and labels\n",
    "\n",
    "path_to_final_project = r'final_project_data'\n",
    "\n",
    "source_images_dir = f'{path_to_final_project}/mos2_preprocessed/'\n",
    "source_labels_dir = f'{path_to_final_project}/mos2_preprocessed/labels/'\n",
    "\n",
    "labeled_images = []\n",
    "tag = 'mos2_preprocessed' \n",
    "\n",
    "for file in os.listdir(source_images_dir):\n",
    "    # find all jpeg file and it's ImageJ label\n",
    "    if file.endswith(\".jpeg\"):\n",
    "        image_path = os.path.join(source_images_dir, file)\n",
    "        label_path = os.path.join(source_labels_dir, file.split('.')[0] + '.txt')\n",
    "        labeled_images.append(labeledImage(image_path))\n",
    "        labeled_images[-1].add_labels_from_file(tag, label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc14b970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 6, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_val_set, test_set = train_test_split(labeled_images, test_size=0.1)\n",
    "train_set, val_set = train_test_split(train_and_val_set, test_size=(2/9))\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bccbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the molecule_images directory if it doesn't exist\n",
    "output_dir = os.path.join(os.getcwd(),'mos2_preprocessed_images')\n",
    "if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
    "\n",
    "train_dir = os.path.join(output_dir, 'train') \n",
    "val_dir   = os.path.join(output_dir, 'val') \n",
    "test_dir  = os.path.join(output_dir, 'test') \n",
    "\n",
    "# Create the sub-directories\n",
    "for d in [train_dir, val_dir, test_dir]:\n",
    "    if not os.path.exists(d): os.mkdir(d)\n",
    "    \n",
    "    images_sub_dir = os.path.join(d, 'images')\n",
    "    labels_sub_dir = os.path.join(d, 'labels')\n",
    "    \n",
    "    for sub_dir in [images_sub_dir, labels_sub_dir]:\n",
    "        if not os.path.exists(sub_dir): os.mkdir(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c935cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully generated labels for image  14_processed.jpeg\n",
      "successfully generated labels for image  2_processed.jpeg\n",
      "successfully generated labels for image  17_processed.jpeg\n",
      "successfully generated labels for image  3_processed.jpeg\n",
      "successfully generated labels for image  28_processed.jpeg\n",
      "successfully generated labels for image  16_processed.jpeg\n",
      "successfully generated labels for image  18_processed.jpeg\n",
      "successfully generated labels for image  7_processed.jpeg\n",
      "successfully generated labels for image  8_processed.jpeg\n",
      "successfully generated labels for image  27_processed.jpeg\n",
      "successfully generated labels for image  9_processed.jpeg\n",
      "successfully generated labels for image  26_processed.jpeg\n",
      "successfully generated labels for image  11_processed.jpeg\n",
      "successfully generated labels for image  4_processed.jpeg\n",
      "successfully generated labels for image  15_processed.jpeg\n",
      "successfully generated labels for image  10_processed.jpeg\n",
      "successfully generated labels for image  24_processed.jpeg\n",
      "successfully generated labels for image  25_processed.jpeg\n",
      "successfully generated labels for image  19_processed.jpeg\n",
      "successfully generated labels for image  21_processed.jpeg\n",
      "successfully generated labels for image  22_processed.jpeg\n",
      "successfully generated labels for image  23_processed.jpeg\n",
      "successfully generated labels for image  12_processed.jpeg\n",
      "successfully generated labels for image  5_processed.jpeg\n",
      "successfully generated labels for image  1_processed.jpeg\n",
      "successfully generated labels for image  20_processed.jpeg\n",
      "successfully generated labels for image  6_processed.jpeg\n",
      "successfully generated labels for image  13_processed.jpeg\n"
     ]
    }
   ],
   "source": [
    "# make unified yolo tags \n",
    "tags = [tag]\n",
    "\n",
    "# zip the dataset\n",
    "dataset = [(train_dir, train_set),(val_dir, val_set),(test_dir, test_set)]\n",
    "\n",
    "for d, s in dataset:\n",
    "    images_sub_dir = os.path.join(d, 'images')\n",
    "    labels_sub_dir = os.path.join(d, 'labels')\n",
    "\n",
    "    # copy over the images\n",
    "    for img in s:\n",
    "        shutil.copyfile(img.path, os.path.join(images_sub_dir, img.name))\n",
    "    \n",
    "    # covert ImageJ labels to yolo format and save it to labels_sub_dir\n",
    "    convert_to_yolo_format(s, labels_sub_dir, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf675ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate yolo yaml file\n",
    "yolo_yaml = os.path.join(output_dir, 'mos2_preprocessed_defect_detection_yolov5.yaml')\n",
    "\n",
    "with open(yolo_yaml, 'w') as yamlout:\n",
    "    yaml.dump(\n",
    "        {'train': train_dir,\n",
    "         'val': val_dir,\n",
    "         'nc': len(tags),\n",
    "         'names': tags},\n",
    "        yamlout,\n",
    "        default_flow_style=None,\n",
    "        sort_keys=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab7944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=./mos2_preprocessed_images/mos2_preprocessed_defect_detection_yolov5.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=5, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
      "YOLOv5 🚀 v7.0-419-gcd44191c Python-3.9.21 torch-2.7.0+cu126 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.1M/14.1M [00:00<00:00, 21.3MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005078125), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/zbyw/MSE544_project_preprocessed/mos2_preprocessed_images/train/labels... 27 images, 0 backgrounds, 0 corrupt: 100%|██████████| 27/27 [00:00<00:\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/zbyw/MSE544_project_preprocessed/mos2_preprocessed_images/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/zbyw/MSE544_project_preprocessed/mos2_preprocessed_images/val/labels... 15 images, 0 backgrounds, 0 corrupt: 100%|██████████| 15/15 [00:00<00:00, \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/zbyw/MSE544_project_preprocessed/mos2_preprocessed_images/val/labels.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.43 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
      "Plotting labels to yolov5/runs/train/exp3/labels.jpg... \n",
      "/home/zbyw/MSE544_project_preprocessed/yolov5/train.py:355: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 5 dataloader workers\n",
      "Logging results to \u001b[1myolov5/runs/train/exp3\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]/home/zbyw/MSE544_project_preprocessed/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1242     0.2134          0        297        640:  17%|█▋        | 1/6 [00:54<04:31, 54.28s/it]/home/zbyw/MSE544_project_preprocessed/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1229     0.1905          0        183        640:  33%|███▎      | 2/6 [01:43<03:24, 51.02s/it]/home/zbyw/MSE544_project_preprocessed/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1228     0.1646          0        117        640:  50%|█████     | 3/6 [02:00<01:47, 35.67s/it]/home/zbyw/MSE544_project_preprocessed/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1233     0.1622          0        206        640:  67%|██████▋   | 4/6 [02:11<00:51, 25.83s/it]/home/zbyw/MSE544_project_preprocessed/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1232     0.1607          0        174        640:  83%|████████▎ | 5/6 [02:20<00:19, 19.89s/it]/home/zbyw/MSE544_project_preprocessed/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1227     0.1556          0         63        640: 100%|██████████| 6/6 [02:26<00:00, 24.45s/it]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 1.000s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:08<00:08,  8.38s/it]WARNING ⚠️ NMS time limit 0.750s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:13<00:00,  6.63s/it]\n",
      "                   all         15        443     0.0117     0.0158    0.00621    0.00265\n",
      "\n",
      "1 epochs completed in 0.046 hours.\n",
      "Optimizer stripped from yolov5/runs/train/exp3/weights/last.pt, 14.4MB\n",
      "Optimizer stripped from yolov5/runs/train/exp3/weights/best.pt, 14.4MB\n",
      "\n",
      "Validating yolov5/runs/train/exp3/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 1.000s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:09<00:09,  9.49s/it]WARNING ⚠️ NMS time limit 0.750s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:16<00:00,  8.44s/it]\n",
      "                   all         15        443       0.01     0.0203    0.00534    0.00218\n",
      "Results saved to \u001b[1myolov5/runs/train/exp3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv5 training with pretrained weights\n",
    "# YOLOv5 will download weights if not found\n",
    "%run yolov5/train.py --img 640 --batch 5 --epochs 1 --data ./mos2_preprocessed_images/mos2_preprocessed_defect_detection_yolov5.yaml --weights yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf8b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d83c30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_env = Environment(name=\"yolov5_env\")\n",
    "\n",
    "# Start from a base docker environments defined by Microsoft\n",
    "yolov5_env.docker.base_image  = \"docker.io/hstirrat/yolov5-env:fixed\"\n",
    "\n",
    "conda_dep = CondaDependencies()\n",
    "# Indicate which version of python needs to be installed\n",
    "conda_dep.add_conda_package('python=3.9')\n",
    "\n",
    "# install all the yolov5 requirement at the image build time\n",
    "with open('./yolov5/requirements.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    \n",
    "    while line != '':    \n",
    "        # If the line is a comment or empty, skip it    \n",
    "        if line.startswith('#') or len(line.split()) == 0:\n",
    "            line = f.readline()\n",
    "            continue\n",
    "        # Otherwise add the corresponding package name as a dependency\n",
    "        conda_dep.add_pip_package(line.split()[0])\n",
    "        # Then move on to the next line in the requirements.txt file\n",
    "        line = f.readline()\n",
    "\n",
    "yolov5_env.python.conda_dependencies=conda_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e18a61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Environment.get_image_details of {\n",
       "    \"assetId\": null,\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"docker.io/hstirrat/yolov5-env:fixed\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": \"2g\"\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"yolov5_env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.9\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"gitpython>=3.1.30\",\n",
       "                        \"matplotlib>=3.3\",\n",
       "                        \"numpy>=1.23.5\",\n",
       "                        \"opencv-python>=4.1.1\",\n",
       "                        \"pillow>=10.3.0\",\n",
       "                        \"psutil\",\n",
       "                        \"PyYAML>=5.3.1\",\n",
       "                        \"requests>=2.32.2\",\n",
       "                        \"scipy>=1.4.1\",\n",
       "                        \"thop>=0.1.1\",\n",
       "                        \"torch>=1.8.0\",\n",
       "                        \"torchvision>=0.9.0\",\n",
       "                        \"tqdm>=4.66.3\",\n",
       "                        \"ultralytics>=8.2.34\",\n",
       "                        \"pandas>=1.1.4\",\n",
       "                        \"seaborn>=0.11.0\",\n",
       "                        \"setuptools>=70.0.0\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"project_environment\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": null\n",
       "}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolov5_env.get_image_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ee4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '6b86c32f-3e1d-4be7-bbd4-dd16443808b2'\n",
    "resource_group  = 'rg-amlclass-zbyw'\n",
    "workspace_name  = 'azureml-zbyw'\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "experiment = Experiment(workspace=ws, name='mos2_preprocessed_defect_detection_yolo_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbbe3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall configuration for the script to be run on the compute cluster\n",
    "config = ScriptRunConfig(source_directory='./deploy_yolo_training/',   ## folder in which the script is located\n",
    "                         script='training_on_aml.py',       ## script name\n",
    "                         compute_target='GPU-zbyw1',\n",
    "                         environment=yolov5_env)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb48acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zbyw/MSE544_project_preprocessed'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984a6f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ml.azure.com/runs/mos2_preprocessed_defect_detection_yolo_training_1748743172_771f95fe?wsid=/subscriptions/6b86c32f-3e1d-4be7-bbd4-dd16443808b2/resourcegroups/rg-amlclass-zbyw/workspaces/azureml-zbyw&tid=f6b6dd5b-f02f-441a-99a0-162ac5060bd2\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(config)\n",
    "aml_url = run.get_portal_url()\n",
    "print(aml_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a8a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
